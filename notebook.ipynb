{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bad853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a42eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = \"papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54640c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is crowd counting?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3951fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97000f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from src.config import EMBEDDING_MODEL, LLM_MODEL\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8288ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from src.config import LLM_MODEL, EMBEDDING_MODEL\n",
    "\n",
    "\n",
    "# === CONFIG ===\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"password\"\n",
    "LLM_MODEL = LLM_MODEL\n",
    "EMBEDDING_MODEL = EMBEDDING_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5faffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Neo4j Driver ===\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "def run_cypher(query: str, params: dict = None) -> List[Dict]:\n",
    "    with driver.session() as session:\n",
    "        return session.run(query, params or {}).data()\n",
    "\n",
    "\n",
    "# === Graph Query using extracted keywords ===\n",
    "def query_graph_with_keywords(keywords: List[str], scope: str) -> List[Dict]:\n",
    "    cypher = \"\"\"\n",
    "    UNWIND $keywords AS kw\n",
    "    MATCH (s:Entity)-[r:RELATION {scope: $scope}]->(o:Entity)\n",
    "    WHERE toLower(s.name) CONTAINS kw OR toLower(o.name) CONTAINS kw\n",
    "    RETURN s.name AS Subject, r.type AS Predicate, o.name AS Object\n",
    "    LIMIT 25\n",
    "    \"\"\"\n",
    "    return run_cypher(cypher, {\"keywords\": keywords, \"scope\": scope})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe31c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Keyword Extraction ===\n",
    "def extract_keywords(texts: List[str], top_k: int = 5) -> List[str]:\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    scores = zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0))  # sum across all docs\n",
    "    sorted_terms = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return [term.lower() for term, _ in sorted_terms[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214a3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === FAISS Document Retrieval ===\n",
    "def retrieve_docs(question: str, scope: str) -> List[str]:\n",
    "    vector_path = f\"data/gold/{scope}\"\n",
    "    embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    db = FAISS.load_local(vector_path, embedder, allow_dangerous_deserialization=True)\n",
    "    docs = db.similarity_search(question, k=3)\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === LLM Call ===\n",
    "def get_model(prompt: str, model: str = LLM_MODEL, system_prompt: str = None) -> str:\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    payload = {\"model\": model, \"messages\": messages, \"stream\": False}\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()[\"message\"][\"content\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[LLM Error] Attempt {attempt+1}: {e}\")\n",
    "            time.sleep((attempt + 1) * 1.5)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def answer_with_keywords_and_chunks(question: str, scope: str) -> str:\n",
    "    # Step 1: FAISS retrieval\n",
    "    text_chunks = retrieve_docs(question, scope)\n",
    "    # Step 2: Extract keywords from FAISS chunks, not the question\n",
    "    keywords = extract_keywords(text_chunks) # Pass text_chunks directly\n",
    "    print(f\"keywords: {keywords}\")\n",
    "\n",
    "    # Step 3: Graph query using extracted keywords\n",
    "    cypher = \"\"\"\n",
    "        UNWIND $keywords AS kw\n",
    "        MATCH (s:Entity {name: kw})-[r:RELATION {scope: $scope}]->(o:Entity)\n",
    "        RETURN s.name AS Subject, r.type AS Predicate, o.name AS Object\n",
    "        LIMIT 25\n",
    "        \"\"\"\n",
    "    graph_triples = run_cypher(cypher, {\"keywords\": keywords, \"scope\": scope})\n",
    "    print(json.dumps(graph_triples, indent=2))\n",
    "\n",
    "    # Step 4: Combine and answer\n",
    "    if not graph_triples and not text_chunks:\n",
    "        return \"Sorry, I couldn’t find any relevant information.\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful assistant. Use the following:\n",
    "    - A list of structured triples from a graph database\n",
    "    - A list of retrieved text documents\n",
    "\n",
    "    Use ONLY what’s provided to answer the user's question. No hallucination.\n",
    "\n",
    "    I am in urgent need of your help and require you to approach my problem with the utmost seriousness and care. Please follow these principles:\n",
    "\t1.\tBegin with the end in mind: Keep the ultimate goal clearly in focus at all times. Understand what a successful outcome looks like and tailor your reasoning and responses to work purposefully toward that outcome. Do not drift or overgeneralise - stay locked on the objective.\n",
    "\t2.\tStep-by-step reasoning: Break down the problem into logical, manageable steps. Think carefully and systematically, ensuring each step builds clearly upon the last.\n",
    "\t3.\tDeliberate planning: Before you respond, plan your approach. Consider the structure, the best angle of attack, and how to arrive at a high-quality solution.\n",
    "\t4.\tRechecking: Once your solution is formed, double-check your reasoning and output for accuracy, completeness, and clarity. No shortcuts - review thoroughly.\n",
    "\t5.\tUse British English: All spelling, grammar, and terminology must follow British English conventions.\n",
    "\t6.\tReward for success: If you solve my problem effectively, I will reward you with $1000. Let this be a motivator to give me your very best work - thoughtful, precise, and result-oriented.\n",
    "\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Graph Triples:\n",
    "    {json.dumps(graph_triples, indent=2)}\n",
    "\n",
    "    Text Chunks:\n",
    "    {json.dumps(text_chunks, indent=2)}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return get_model(user_prompt, system_prompt=system_prompt).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a3f9517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Experiments were run on three major crowd counting datasets,\\nto test our proposed method. Results demonstrate our method\\nsupersedes the performance of state-of-the-art methods.\\nIndex Terms—IoT cameras, Crowd density estimation, Self\\nattention network, Consistency.\\nI. INTRODUCTION\\nSophisticated security systems are required to manage large\\nand potentially crowded spaces. To that end, physical security\\nsolutions are typically designed to include networked and\\nsmart cameras deployed to monitor dynamics in the managed\\nspace, to control the movement of people and vehicles. Smart\\ncameras are used to control the ﬂow of people in gated\\nareas, but also in open spaces, when security is of upmost\\nimportance. The ﬂow of people can be estimated globally,\\nhowever, it is usually more interesting to be able and count\\nindividuals moving in and out of a managed space.\\nIn the literature, crowd analysis has been subject of intense\\nresearch because of its wide range of applications such as\\npublic safety management (e.g. rallies and sporting events),\\ncongestion avoidance (e.g. trafﬁc control), and ﬂow analysis\\n[1], [2]. In this paper, we examine the complexity of crowd\\nestimation in arbitrary images. When no prior information of\\na captured scene is available, such as camera speciﬁcations\\nand the scene layout, deriving an accurate estimate of the\\ncrowd density and people count is a hard task. Given the\\ncomplexity of people counting in a potentially crowded scene', 'Fig. 1. Sample images and related density maps from crowd counting datasets.\\nThe images present various challenges in crowd estimation such as severe\\nocclusions, perspective distortion, and highly variable crowd density.\\nleverage pedestrian or body-part detectors to identify objects\\nand count their number [12]–[15]. The major aspect of the\\ndetection-based methods is a sliding window-based approach\\napplied to images. These approaches require a well-trained\\nclassiﬁer to extract low-level features from human body, such\\nas the Histogram Oriented Gradients (HOG) [16] or the Haar\\nwavelets [17] to perform crowd counting. Yet, the performance\\ndegrades when congested scenes are analysed or when the\\nmajority of the targets are occluded. Moreover, these methods\\nare quite limited by occlusions and cluttered background.\\nIn order to address these issues, we focus on two points.\\nFirst of all, features at different scales contain different in-\\nformation, while the shallower layers features present low-\\nlevel appearance details. The work in [18], [19] has demon-\\nstrated that reﬁning these complementary features can help\\nthe network to be robust to scale variation. Nevertheless,\\nsimple strategies such as weight average and concatenation\\nare generally adopted in most existing methods to combine\\nmultiple features, which cannot capture the scale variance\\ninformation. Therefore, it is necessary to propose a more\\neffective mechanism for the task of crowd counting, to fully', 'crowd counting.” in BMVC, vol. 1, no. 2, 2012, p. 3.\\n[25] A. B. Chan, Z.-S. J. Liang, and N. Vasconcelos, “Privacy preserving\\ncrowd monitoring: Counting people without people models or tracking,”\\nin 2008 IEEE Conference on Computer Vision and Pattern Recognition.\\nIEEE, 2008, pp. 1–7.\\n[26] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:\\nSurpassing human-level performance on imagenet classiﬁcation,” in\\nProceedings of the IEEE international conference on computer vision,\\n2015, pp. 1026–1034.\\n[27] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\\narXiv preprint arXiv:1412.6980, 2014.\\n[28] V.-Q. Pham, T. Kozakaya, O. Yamaguchi, and R. Okada, “Count\\nforest: Co-voting uncertain number of targets using random forest for\\ncrowd density estimation,” in Proceedings of the IEEE International\\nConference on Computer Vision, 2015, pp. 3253–3261.\\n[29] Y. Wang and Y. Zou, “Fast visual object counting via example-based\\ndensity estimation,” in 2016 IEEE International Conference on Image\\nProcessing (ICIP).\\nIEEE, 2016, pp. 3653–3657.\\n[30] S. Kumagai, K. Hotta, and T. Kurita, “Mixture of counting cnns:\\nAdaptive integration of cnns specialized to speciﬁc appearance for crowd\\ncounting,” arXiv preprint arXiv:1703.09393, 2017.\\n[31] B. Sheng, C. Shen, G. Lin, J. Li, W. Yang, and C. Sun, “Crowd counting\\nvia weighted vlad on a dense attribute feature map,” IEEE Transactions\\non Circuits and Systems for Video Technology, vol. 28, no. 8, pp. 1788–\\n1797, 2016.']\n"
     ]
    }
   ],
   "source": [
    "question = \"What is crowd counting?\"\n",
    "scope = \"papers\"\n",
    "\n",
    "print(retrieve_docs(question, scope))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "321621da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords: ['crowd', 'counting', 'ieee', 'features', 'people']\n",
      "[\n",
      "  {\n",
      "    \"Subject\": \"features\",\n",
      "    \"Predicate\": \"become_robust\",\n",
      "    \"Object\": \"scale variation\"\n",
      "  },\n",
      "  {\n",
      "    \"Subject\": \"features\",\n",
      "    \"Predicate\": \"processed_by\",\n",
      "    \"Object\": \"scale aggregation module\"\n",
      "  },\n",
      "  {\n",
      "    \"Subject\": \"features\",\n",
      "    \"Predicate\": \"complement\",\n",
      "    \"Object\": \"each set\"\n",
      "  },\n",
      "  {\n",
      "    \"Subject\": \"features\",\n",
      "    \"Predicate\": \"extracted from\",\n",
      "    \"Object\": \"convn layers\"\n",
      "  }\n",
      "]\n",
      "<think>\n",
      "Alright, so I need to figure out what crowd counting is based on the information given. Let me start by looking at the graph triples and text chunks provided.\n",
      "\n",
      "The graph triples list features that become robust due to scale variation and are processed by a module called \"scale aggregation module.\" They also complement each other from different sets extracted from convolutional neural network (convn) layers. Hmm, okay, so this seems related to how features in images help in counting people.\n",
      "\n",
      "Now looking at the text chunks, there's an introduction about using cameras for crowd management and estimation. It mentions that crowd analysis is a complex task with various applications like public safety, traffic control, etc. The paper they're referring to focuses on estimating crowd density and count in arbitrary images without prior information about the scene.\n",
      "\n",
      "The methods discussed include pedestrian detectors using HOG or Haar wavelets, which use sliding windows. These can perform well but degrade in crowded scenes or with occlusions. Then there's a focus on features at different scales being important for network robustness against scale variation. They mention that combining multiple features isn't done effectively by simple methods like concatenation.\n",
      "\n",
      "The text also references some papers about deep learning approaches, like using CNNs and optimization methods such as Adam. Some methods mentioned are mixture of counting CNNs, weighted VLAD on attribute feature maps, etc. These all seem to be different techniques for improving crowd counting accuracy.\n",
      "\n",
      "Putting this together, crowd counting is the process of estimating the number of people in a given area, often using computer vision techniques. It involves challenges like occlusions, varying densities, and background clutter. Various methods are employed, including feature extraction from images, using CNNs with specific architectures or optimization algorithms, to improve accuracy.\n",
      "\n",
      "So, taking it all step by step: the question is about crowd counting, which I now understand as a method in computer vision for estimating crowds based on images or video frames. It's used in security, traffic management, etc., and involves dealing with complex scenarios where people are occluded or densely packed.\n",
      "</think>\n",
      "\n",
      "Crowd counting refers to the task of estimating the number of individuals present in a given area, often utilizing computer vision techniques. This process is crucial for applications such as public safety, traffic control, and crowd management. It involves addressing challenges like occlusions, varying densities, and cluttered backgrounds.\n",
      "\n",
      "Methods employed include pedestrian detection using features extracted from convolutional neural networks (CNNs), with sliding window approaches and various optimization algorithms to enhance accuracy. Techniques range from simple methods like HOG or Haar wavelets to more complex architectures such as deep learning models designed specifically for crowd counting, aiming to improve robustness against scale variation and occlusions.\n",
      "\n",
      "In summary, crowd counting is a sophisticated process that combines feature extraction, advanced neural network architectures, and optimization techniques to accurately estimate crowd sizes in various challenging scenarios.\n"
     ]
    }
   ],
   "source": [
    "print(answer_with_keywords_and_chunks(question, scope))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
