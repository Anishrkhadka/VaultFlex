{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VaultFlex","text":"<p>Chat with your Knowledge using a hybrid RAG pipeline (Vector + Graph). Built for teams that want full control over their data.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>\ud83d\udcc1 Flexible Knowledge Base Scopes \u2014 create isolated workspaces per domain or project</li> <li>\ud83e\udde0 Hybrid Retrieval \u2014 combines vector search (FAISS) and knowledge graphs (Neo4j)</li> <li>\ud83d\udd0d LLM-Augmented Reasoning \u2014 responds only from your ingested content (no hallucinations)</li> <li>\ud83e\uddfe Document Parsing \u2014 PDF, DOCX, Markdown, TXT support</li> <li>\ud83e\ude84 Interactive UI \u2014 powered by Streamlit, clean and customisable</li> <li>\ud83d\udcbe Medallion Data Architecture \u2014 Bronze \u2192 Silver \u2192 Gold data layers</li> </ul>"},{"location":"#medallion-data-architecture","title":"\ud83d\udcc1 Medallion Data Architecture","text":"<p>VaultFlex follows a structured Medallion data pipeline to process documents:</p> <ul> <li>\ud83e\udd49 Bronze: Raw files uploaded by the user are stored.</li> <li>\ud83e\udd48 Silver: Cleaned, chunked, and standardised text representations (JSON).</li> <li>\ud83e\udd47 Gold: Semantic embeddings stored in FAISS, and knowledge graphs built in Neo4j.</li> </ul> <pre><code>VaultFlex/\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 &lt;dataset_name&gt;/\n\u2502       \u251c\u2500\u2500 bronze/       # Raw documents (PDF, DOCX, etc.)\n\u2502       \u251c\u2500\u2500 silver/       # Chunked and cleaned JSON text\n\u2502       \u2514\u2500\u2500 gold/         # FAISS vector store (and in Neo4j)\n</code></pre>"},{"location":"#smart-upload-hash-based-deduplication","title":"\ud83d\udee1\ufe0f Smart Upload: Hash-Based Deduplication","text":"<p>VaultFlex automatically prevents duplicate document ingestion:</p> <ul> <li>\u2705 On upload, each file is hashed (e.g., SHA256).</li> <li>\ud83d\udeab If the same file (even renamed) was previously uploaded, it's rejected.</li> <li>\ud83d\udcbe This ensures clean, efficient, and consistent data processing.</li> </ul> <p>Hashes are tracked in a persistent file: <code>HASH_TRACK_FILE</code>, with scope-aware keys like <code>finance/employee_policy.pdf</code>.</p>"},{"location":"#technologies","title":"\ud83e\uddea Technologies","text":"<ul> <li><code>Streamlit</code> \u2014 UI</li> <li><code>FAISS</code> + <code>HuggingFace</code> \u2014 Embedding &amp; vector storage</li> <li><code>Neo4j</code> \u2014 Knowledge graph backend</li> <li><code>LangChain</code> \u2014 Document parsing and chunking</li> <li><code>Ollama</code> / <code>LLM API</code> \u2014 Model communication</li> </ul>"},{"location":"#project-structure","title":"\ud83d\udce6 Project Structure","text":"<pre><code>VaultFlex/\n\u251c\u2500\u2500 data/                    # All datasets (Bronze/Silver/Gold)\n\u2502   \u251c\u2500\u2500 bronze/             # Raw uploaded documents per KB\n\u2502   \u251c\u2500\u2500 silver/             # Chunked documents as JSON\n\u2502   \u251c\u2500\u2500 gold/               # FAISS vector indexes\n\u2502   \u2514\u2500\u2500 ingested_hashes.json\n\u251c\u2500\u2500 doc/                    # Docs and branding\n\u2502   \u2514\u2500\u2500 images/vaultFlex_logo.png\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 ui/                 # Streamlit UI modules\n\u2502   \u251c\u2500\u2500 vector/             # Embedding, graph builder, retriever\n\u2502   \u251c\u2500\u2500 utils/              # Utility tools (e.g. service checks)\n\u2502   \u251c\u2500\u2500 config.py           # Centralised paths and settings\n\u2502   \u2514\u2500\u2500 __version__.py\n\u251c\u2500\u2500 main.py                 # App entry point\n\u251c\u2500\u2500 environment.yml         # Conda environment setup\n\u251c\u2500\u2500 requirements.txt        # Pip requirements\n\u2514\u2500\u2500 README.md               # You're reading it\n\n</code></pre>"},{"location":"#pipeline-workflow","title":"\ud83d\udd01 Pipeline Workflow","text":"<pre><code>flowchart TD\n    %% UI Layer\n    subgraph UI [\"\ud83d\udda5\ufe0f Streamlit UI\"]\n        A[\"\ud83d\udcc4 Upload Docs\"] --&gt; B[\"\ud83e\udded Choose Dataset Scope\"]\n        B --&gt; C[\"\ud83d\udccc Select LLM\"]\n        C --&gt; D[\"\ud83d\udcac Ask Question\"]\n    end\n\n    %% Bronze Layer\n    subgraph Bronze [\"\ud83e\udd49 Bronze Layer \u2013 Raw Documents\"]\n        B --&gt; E[\"\ud83e\uddf9 Preprocess + Dedup\"]\n        E --&gt; F[\"\ud83d\udcbe Store Raw Files\"]\n    end\n\n    %% Silver Layer\n    subgraph Silver [\"\ud83e\udd48 Silver Layer \u2013 Cleaned Chunks\"]\n        F --&gt; G[\"\ud83e\udde9 Text Chunking\"]\n        G --&gt; H[\"\ud83d\udcc4 Store Cleaned Chunks (JSON)\"]\n    end\n\n    %% Gold Layer\n    subgraph Gold [\"\ud83e\udd47 Gold Layer \u2013 Indexes &amp; Knowledge\"]\n        G --&gt; I[\"\ud83d\udd0e Generate Embeddings (FAISS)\"]\n        G --&gt; J[\"\ud83e\udde0 Extract Triples (Entities &amp; Relations)\"]\n        I --&gt; K[\"\ud83d\udcda FAISS Index\"]\n        J --&gt; L[\"\ud83d\udd78\ufe0f Neo4j Graph DB\"]\n    end\n\n    %% Query Flow\n    D --&gt; Q[\"\ud83e\udde0 Enhance Query via LLM\"]\n    Q --&gt; M1[\"\ud83d\udd0d Retrieve Context from FAISS\"]\n    Q --&gt; M2[\"\ud83c\udf10 Query Neo4j (GraphRAG)\"]\n    M1 --&gt; N[\"\ud83e\udde0 LLM Generates Final Answer\"]\n    M2 --&gt; N\n\n    %% Styling\n    classDef bronze fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n    classDef silver fill:#e3f2fd,stroke:#1565c0,stroke-width:2px\n    classDef gold fill:#fff8e1,stroke:#f9a825,stroke-width:2px\n    classDef ui fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px\n    classDef query fill:#ede7f6,stroke:#512da8,stroke-width:2px\n    classDef enhance fill:#fce4ec,stroke:#ad1457,stroke-width:2px\n\n    class UI ui\n    class Bronze bronze\n    class Silver silver\n    class Gold gold\n    class Q enhance\n    class M1,M2,N query\n\n\n</code></pre>"},{"location":"#llm-usage","title":"\ud83e\udde0 LLM Usage","text":"<p>VaultFlex calls your selected local LLM to: - Refine vague user questions - Synthesise answers from graph triples and chunks - Avoid hallucination by grounding output in source data</p> <p>Supports: <code>deepseek-r1</code>, <code>gemma3</code>, or anything via <code>Ollama</code>.</p>"},{"location":"#quickstart","title":"\ud83c\udfc1 Quickstart","text":"<pre><code># 1. Start backend (Ollama and neo4j)\n# 2. Install dependencies\npip install -r requirements.txt\n\n# 3. Update .env (see env_template)\nupdate .env\n\n# 4. Launch the UI\nstreamlit run app.py\n</code></pre>"},{"location":"#example-use","title":"\ud83d\udcac Example Use","text":"<ul> <li>Upload multiple PDFs and DOCXs into <code>Finance</code> knowledge base</li> <li>Ask: \"what were the key themes discussed in quarterly reports?\"</li> <li>VaultFlex retrieves vector data, queries the graph, and answers using both</li> </ul> <p>\ud83e\udd1d Made with \u2764\ufe0f by Anish Khadka - VaultFlex</p>"},{"location":"reference/chat_ui/","title":"<code>chat_ui.py</code>","text":"<p>chat_ui.py</p> <p>This module defines the Streamlit-based chat interface for VaultFlex. It allows users to ask questions against an indexed knowledge base (KB), and receive answers via an LLM-powered retrieval system.</p> <p>Key components: - Streamlit UI to display and handle chat interactions - Scoped knowledge base selection - Retrieval using FAISS (vector) and optional graph context</p>"},{"location":"reference/chat_ui/#src.frontent.chat_ui.run_chat_ui","title":"<code>run_chat_ui()</code>","text":"<p>Launches the VaultFlex chat interface via Streamlit.</p> <p>This function: - Loads the selected KB and model from the session - Validates the availability of the KB - Manages chat history state - Displays previous messages - Captures user input and generates responses using <code>KnowledgeBaseRetriever</code></p>"},{"location":"reference/embedder/","title":"<code>embedder.py</code>","text":"<p>embedder.py</p> <p>Defines the <code>KnowledgeBaseIngestor</code> class, which handles the ingestion pipeline for a single knowledge base (scope) in VaultFlex.</p> <p>Responsibilities: - Load and hash uploaded documents (bronze layer) - Chunk documents into segments (silver layer) - Generate and persist embeddings using FAISS (gold layer) - Extract semantic triples via LLM and store in Neo4j</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor","title":"<code>KnowledgeBaseIngestor</code>","text":"<p>Main class to orchestrate the ingestion of documents into VaultFlex's medallion pipeline.</p> <p>This includes: - File deduplication via hashing - Document loading and chunking - FAISS embedding index creation - Graph-based triple extraction via LLM</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.__init__","title":"<code>__init__(scope)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>scope</code> <code>str</code> <p>Name of the knowledge base or dataset</p> required"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.build_graph","title":"<code>build_graph(chunks)</code>","text":"<p>Run knowledge graph extraction via LLM and push to Neo4j.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list</code> <p>Chunked documents to extract triples from</p> required"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.chunk_only","title":"<code>chunk_only()</code>","text":"<p>Run only the loading and chunking stages.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of text chunks (LangChain Document format)</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.embed_only","title":"<code>embed_only(chunks)</code>","text":"<p>Run only the embedding and FAISS storage step.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list</code> <p>Chunked documents</p> required"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.get_file_hash","title":"<code>get_file_hash(file_obj)</code>","text":"<p>Compute SHA-256 hash of a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_obj</code> <code>Path or file - like</code> <p>Local file or Streamlit upload</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Hexadecimal hash string</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.ingest","title":"<code>ingest()</code>","text":"<p>Full ingestion pipeline: - Load and deduplicate documents - Chunk and save cleaned text - Embed and index into FAISS - Extract triples and build graph</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.is_already_ingested","title":"<code>is_already_ingested(file_path)</code>","text":"<p>Check if the file has already been processed by comparing stored hashes.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file is already ingested</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.load_documents","title":"<code>load_documents()</code>","text":"<p>Load all new documents from the bronze layer, skipping duplicates.</p> <p>Returns:</p> Type Description <code>list</code> <p>list[Document]: LangChain-compatible document objects</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.save_chunks_to_silver","title":"<code>save_chunks_to_silver(chunks)</code>","text":"<p>Persist cleaned chunks as JSON to the silver layer.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list</code> <p>Chunked documents to save</p> required"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.split_documents","title":"<code>split_documents(docs)</code>","text":"<p>Split documents into overlapping chunks.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list</code> <p>List of LangChain Document objects</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of chunked documents</p>"},{"location":"reference/embedder/#src.vector.embedder.KnowledgeBaseIngestor.store_embeddings","title":"<code>store_embeddings(chunks)</code>","text":"<p>Generate vector embeddings and save them as a FAISS index.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list</code> <p>List of chunked documents</p> required"},{"location":"reference/file_utils/","title":"<code>file_utils.py</code>","text":"<p>file_utils.py</p> <p>Utility functions for file handling within the VaultFlex ingestion pipeline.</p> <p>Responsibilities: - File deduplication via SHA-256 hashing - Scope discovery (from folder structure) - Determining which uploaded files have already been ingested</p>"},{"location":"reference/file_utils/#src.utils.file_utils.check_ingested_status","title":"<code>check_ingested_status(scope_name, files, hash_file=HASH_TRACK_FILE)</code>","text":"<p>Check which uploaded files are new vs. already ingested (based on hash tracking).</p> <p>Parameters:</p> Name Type Description Default <code>scope_name</code> <code>str</code> <p>The dataset scope (e.g., \"hr_docs\").</p> required <code>files</code> <code>List[UploadedFile]</code> <p>List of files uploaded via Streamlit.</p> required <code>hash_file</code> <code>Path</code> <p>Path to the JSON file that stores ingested hashes. Defaults to HASH_TRACK_FILE.</p> <code>HASH_TRACK_FILE</code> <p>Returns:</p> Type Description <code>Tuple[List[str], List[BinaryIO]]</code> <p>Tuple[List[str], List[UploadedFile]]: - List of filenames already ingested - List of UploadedFile objects that are new</p>"},{"location":"reference/file_utils/#src.utils.file_utils.get_existing_scopes","title":"<code>get_existing_scopes(bronze_dir)</code>","text":"<p>List all existing knowledge base scopes (directories) in the bronze layer.</p> <p>Parameters:</p> Name Type Description Default <code>bronze_dir</code> <code>Path</code> <p>Path to the 'bronze' directory.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Sorted list of scope names (directory names).</p>"},{"location":"reference/file_utils/#src.utils.file_utils.get_file_hash","title":"<code>get_file_hash(file_obj)</code>","text":"<p>Compute a SHA-256 hash for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>file_obj</code> <code>Path or file-like object</code> <p>File to hash. Can be a local Path or Streamlit UploadedFile.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Hexadecimal SHA-256 hash string.</p>"},{"location":"reference/kb_ingest_ui/","title":"<code>kb_ingest_ui.py</code>","text":"<p>kb_ingest_ui.py</p> <p>This module defines the Streamlit interface for uploading, ingesting, and deleting documents into a structured knowledge base within VaultFlex.</p> <p>Features: - Upload documents into a named scope (KB) - Automatically preprocess, chunk, embed (FAISS), and extract triples (Neo4j) - Skip already-ingested files using hash-based deduplication - Support for deleting all files, vectors, and graph nodes for a given scope</p>"},{"location":"reference/kb_ingest_ui/#src.frontent.kb_ingest_ui.delete_scope","title":"<code>delete_scope(scope_name)</code>","text":"<p>Deletes all resources associated with a given scope (knowledge base): - Raw files (bronze), cleaned chunks (silver), and embeddings (gold) - Entries from the HASH_TRACK_FILE - Nodes and relationships from Neo4j with matching scope</p> <p>Parameters:</p> Name Type Description Default <code>scope_name</code> <code>str</code> <p>The scope (KB) name to delete.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of hash entries removed from the HASH_TRACK_FILE</p>"},{"location":"reference/kb_ingest_ui/#src.frontent.kb_ingest_ui.run_ingestion_ui","title":"<code>run_ingestion_ui()</code>","text":"<p>Renders the Streamlit UI for uploading and ingesting documents into a KB.</p> <p>Steps: 1. User selects or creates a scope (knowledge base). 2. User uploads one or more supported files. 3. System checks for duplicates via file hash. 4. New files are:     - Stored in the bronze layer     - Chunked and embedded into FAISS     - Used to build a graph in Neo4j 5. Optional: user can delete the entire scope and its assets.</p>"},{"location":"reference/llm_graph_builder/","title":"<code>llm_graph_builder.py</code>","text":"<p>llm_graph_builder.py</p> <p>This module defines <code>GraphBuilderLLM</code>, a class responsible for extracting semantic triples (subject\u2013predicate\u2013object) from text using an LLM, and inserting them into a scoped Neo4j graph.</p> <p>The class supports: - Querying a local LLM (e.g., via Ollama) for triple extraction - Inserting nodes and relationships with scope tagging into Neo4j - Processing batches of text chunks from a knowledge base</p>"},{"location":"reference/llm_graph_builder/#src.vector.llm_graph_builder.GraphBuilderLLM","title":"<code>GraphBuilderLLM</code>","text":"<p>Handles LLM-based semantic triple extraction and inserts them into a Neo4j graph.</p>"},{"location":"reference/llm_graph_builder/#src.vector.llm_graph_builder.GraphBuilderLLM.__init__","title":"<code>__init__(uri=NEO4J_URI, user=NEO4J_USER, password=NEO4J_PASSWORD, model=LLM_MODEL_EMBEDDING_MODEL)</code>","text":"<p>Initialise the GraphBuilderLLM with Neo4j connection and LLM model config.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>Bolt URI for Neo4j.</p> <code>NEO4J_URI</code> <code>user</code> <code>str</code> <p>Neo4j username.</p> <code>NEO4J_USER</code> <code>password</code> <code>str</code> <p>Neo4j password.</p> <code>NEO4J_PASSWORD</code> <code>model</code> <code>str</code> <p>LLM model name to call via Ollama.</p> <code>LLM_MODEL_EMBEDDING_MODEL</code>"},{"location":"reference/llm_graph_builder/#src.vector.llm_graph_builder.GraphBuilderLLM.close","title":"<code>close()</code>","text":"<p>Closes the Neo4j database connection.</p>"},{"location":"reference/llm_graph_builder/#src.vector.llm_graph_builder.GraphBuilderLLM.extract_triples_with_llm","title":"<code>extract_triples_with_llm(text, max_retries=1, backoff_secs=1.0)</code>","text":"<p>Sends a text block to an LLM and extracts semantic triples.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to analyse.</p> required <code>max_retries</code> <code>int</code> <p>Number of retry attempts on failure.</p> <code>1</code> <code>backoff_secs</code> <code>float</code> <p>Delay multiplier between retries.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict[str, str]]: List of valid triples in dict form.</p>"},{"location":"reference/llm_graph_builder/#src.vector.llm_graph_builder.GraphBuilderLLM.insert_triple","title":"<code>insert_triple(subject, predicate, obj, scope)</code>","text":"<p>Inserts a semantic triple into Neo4j with scope tagging.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>str</code> <p>Triple subject.</p> required <code>predicate</code> <code>str</code> <p>Triple predicate.</p> required <code>obj</code> <code>str</code> <p>Triple object.</p> required <code>scope</code> <code>str</code> <p>Knowledge base scope to isolate data.</p> required"},{"location":"reference/llm_graph_builder/#src.vector.llm_graph_builder.GraphBuilderLLM.process_chunks","title":"<code>process_chunks(chunks, scope)</code>","text":"<p>Processes a list of text chunks: - Sends each chunk to the LLM to extract triples - Inserts all valid triples into the Neo4j graph</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list</code> <p>List of LangChain Document objects with <code>.page_content</code>.</p> required <code>scope</code> <code>str</code> <p>The current knowledge base scope.</p> required"},{"location":"reference/retriever/","title":"<code>retriever.py</code>","text":"<p>retriever.py</p> <p>This module defines the <code>KnowledgeBaseRetriever</code> class, which is responsible for: - Rewriting user queries via LLM for clarity and specificity - Retrieving semantically relevant chunks using FAISS (dense retrieval) - Extracting keywords for graph queries - Running Cypher queries over scoped Neo4j subgraphs - Generating answers using a local LLM based on retrieved text and triples</p>"},{"location":"reference/retriever/#src.vector.retriever.KnowledgeBaseRetriever","title":"<code>KnowledgeBaseRetriever</code>","text":"<p>Retrieves information from FAISS and Neo4j based on user questions. Enhances queries using LLM and returns factual answers.</p>"},{"location":"reference/retriever/#src.vector.retriever.KnowledgeBaseRetriever.answer_with_keywords_and_chunks","title":"<code>answer_with_keywords_and_chunks(question, scope, model_name)</code>","text":"<p>Full RAG pipeline: Rewrites question, retrieves chunks, queries graph, and generates an answer.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The user input.</p> required <code>scope</code> <code>str</code> <p>The knowledge base scope.</p> required <code>model_name</code> <code>str</code> <p>Which LLM to use.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Final assistant answer string.</p>"},{"location":"reference/retriever/#src.vector.retriever.KnowledgeBaseRetriever.extract_keywords","title":"<code>extract_keywords(texts, top_k=5)</code>","text":"<p>Extracts top-k keywords from a list of documents using TF-IDF.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>Document strings.</p> required <code>top_k</code> <code>int</code> <p>Number of keywords to extract.</p> <code>5</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of keywords.</p>"},{"location":"reference/retriever/#src.vector.retriever.KnowledgeBaseRetriever.retrieve_docs","title":"<code>retrieve_docs(question, scope)</code>","text":"<p>Uses FAISS to retrieve top-k documents relevant to the question.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The query string.</p> required <code>scope</code> <code>str</code> <p>The knowledge base scope.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of relevant text chunks.</p>"},{"location":"reference/retriever/#src.vector.retriever.KnowledgeBaseRetriever.rewrite_question","title":"<code>rewrite_question(question, model_name)</code>","text":"<p>Rewrites a user query for clarity using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The original question.</p> required <code>model_name</code> <code>str</code> <p>LLM model to use.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Rewritten question as plain text.</p>"},{"location":"reference/service_status/","title":"<code>service_status.py</code>","text":"<p>service_status.py</p> <p>Health-check utilities for VaultFlex backend services.</p> <p>Responsibilities: - Verify availability of the Ollama LLM server - Test connectivity to the Neo4j graph database - Report status of configured LLM model</p>"},{"location":"reference/service_status/#src.utils.service_status.check_neo4j","title":"<code>check_neo4j()</code>","text":"<p>Test connectivity to the Neo4j graph database.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the connection is successful, False otherwise.</p>"},{"location":"reference/service_status/#src.utils.service_status.check_ollama","title":"<code>check_ollama()</code>","text":"<p>Check whether the local Ollama API is reachable and responding.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if Ollama is reachable, False otherwise.</p>"},{"location":"reference/service_status/#src.utils.service_status.get_backend_status","title":"<code>get_backend_status()</code>","text":"<p>Return the health status of critical backend services.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing: - 'ollama': True/False for Ollama status - 'neo4j' : True/False for Neo4j status - 'model' : Name of the active LLM model (from config)</p>"}]}